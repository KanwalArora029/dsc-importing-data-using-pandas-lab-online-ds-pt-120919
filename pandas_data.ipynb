{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Skip to content\n",
    "Search or jump to…\n",
    "\n",
    "Pull requests\n",
    "Issues\n",
    "Marketplace\n",
    "Explore\n",
    " \n",
    "@KanwalArora029 \n",
    "learn-co-curriculum\n",
    "/\n",
    "dsc-importing-data-using-pandas-lab\n",
    "30\n",
    "03\n",
    " Code Issues 0 Pull requests 0 Actions Projects 0 Wiki Security Insights\n",
    "No description, website, or topics provided.\n",
    " 14 commits\n",
    " 3 branches\n",
    " 0 packages\n",
    " 0 releases\n",
    " 6 contributors\n",
    " View license\n",
    " Pull request  CompareThis branch is 12 commits ahead, 15 commits behind master.\n",
    "@sumedh10\n",
    "sumedh10 update readme\n",
    "Latest commit\n",
    "40decd0\n",
    "on 15 Oct 2019\n",
    "Type\tName\tLatest commit message\tCommit time\n",
    "Data\tmaster upload\tlast year\n",
    ".gitignore\tinitial commit with learn-create\tlast year\n",
    ".learn\tsolution\tlast year\n",
    "CONTRIBUTING.md\tinitial commit with learn-create\tlast year\n",
    "LICENSE.md\tinitial commit with learn-create\tlast year\n",
    "README.md\tupdate readme\t3 months ago\n",
    "index.ipynb\tupdate readme\t3 months ago\n",
    " README.md\n",
    "Importing Data Using Pandas - Lab\n",
    "Introduction\n",
    "In this lab, you'll get some practice with loading files with summary or metadata, and if you find that easy, the optional \"level up\" content covers loading data from a corrupted csv file!\n",
    "\n",
    "Objectives\n",
    "You will be able to:\n",
    "\n",
    "Use pandas to import data from a CSV and and an Excel spreadsheet\n",
    "Loading Files with Summary or Meta Data\n",
    "Load either of the files 'Zipcode_Demos.csv' or 'Zipcode_Demos.xlsx'. What's going on with this dataset? Clean it up into a useable format and describe the nuances of how the data is currently formatted.\n",
    "\n",
    "All data files are stored in a folder titled 'Data'.\n",
    "\n",
    "# Import pandas using the standard alias\n",
    "import pandas as pd\n",
    "# Import the file and print the first 5 rows\n",
    "df = pd.read_csv('Data/Zipcode_Demos.csv')\n",
    "df.head()\n",
    "<style scoped> .dataframe tbody tr th:only-of-type { vertical-align: middle; }\n",
    ".dataframe tbody tr th {\n",
    "    vertical-align: top;\n",
    "}\n",
    "\n",
    ".dataframe thead th {\n",
    "    text-align: right;\n",
    "}\n",
    "</style>\n",
    "0\tAverage Statistics\tUnnamed: 2\tUnnamed: 3\tUnnamed: 4\tUnnamed: 5\tUnnamed: 6\tUnnamed: 7\tUnnamed: 8\tUnnamed: 9\t...\tUnnamed: 37\tUnnamed: 38\tUnnamed: 39\tUnnamed: 40\tUnnamed: 41\tUnnamed: 42\tUnnamed: 43\tUnnamed: 44\tUnnamed: 45\tUnnamed: 46\n",
    "0\t1\tNaN\t0\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\t...\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "1\t2\tJURISDICTION NAME\t10005.8\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\t...\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "2\t3\tCOUNT PARTICIPANTS\t9.4\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\t...\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "3\t4\tCOUNT FEMALE\t4.8\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\t...\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "4\t5\tPERCENT FEMALE\t0.404\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\t...\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "5 rows × 47 columns\n",
    "\n",
    "# Print the last 5 rows of df\n",
    "df.tail()\n",
    "<style scoped> .dataframe tbody tr th:only-of-type { vertical-align: middle; }\n",
    ".dataframe tbody tr th {\n",
    "    vertical-align: top;\n",
    "}\n",
    "\n",
    ".dataframe thead th {\n",
    "    text-align: right;\n",
    "}\n",
    "</style>\n",
    "0\tAverage Statistics\tUnnamed: 2\tUnnamed: 3\tUnnamed: 4\tUnnamed: 5\tUnnamed: 6\tUnnamed: 7\tUnnamed: 8\tUnnamed: 9\t...\tUnnamed: 37\tUnnamed: 38\tUnnamed: 39\tUnnamed: 40\tUnnamed: 41\tUnnamed: 42\tUnnamed: 43\tUnnamed: 44\tUnnamed: 45\tUnnamed: 46\n",
    "52\t53\t10006\t6\t2\t0.33\t4\t0.67\t0\t0\t6\t...\t6\t100\t0\t0\t6\t1\t0\t0\t6\t100\n",
    "53\t54\t10007\t1\t0\t0\t1\t1\t0\t0\t1\t...\t1\t100\t1\t1\t0\t0\t0\t0\t1\t100\n",
    "54\t55\t10009\t2\t0\t0\t2\t1\t0\t0\t2\t...\t2\t100\t0\t0\t2\t1\t0\t0\t2\t100\n",
    "55\t56\t10010\t0\t0\t0\t0\t0\t0\t0\t0\t...\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
    "56\t57\t10011\t3\t2\t0.67\t1\t0.33\t0\t0\t3\t...\t3\t100\t0\t0\t3\t1\t0\t0\t3\t100\n",
    "5 rows × 47 columns\n",
    "\n",
    "# Comment: Dataframe is really two table views, one on top of the other. \n",
    "# The first is a summary view of the raw data below. \n",
    "# There is also a blank row at row 1 in the file.\n",
    "# Clean up the dataset\n",
    "prev_count = 10**3\n",
    "for row in df.index:\n",
    "    count = 0\n",
    "    for entry in df.iloc[row].isnull():\n",
    "        if entry:\n",
    "            count += 1\n",
    "    if count != prev_count and row!=0:\n",
    "        print('On row {} there are {} null values. The previous row had {} null values.'.format(row, count, prev_count))\n",
    "    prev_count = count\n",
    "On row 1 there are 44 null values. The previous row had 45 null values.\n",
    "On row 46 there are 0 null values. The previous row had 44 null values.\n",
    "# Import the first part of the data\n",
    "df1 = pd.read_csv('Data/Zipcode_Demos.csv', skiprows=[1], nrows=45, usecols=[0, 1, 2])\n",
    "df1.head()\n",
    "<style scoped> .dataframe tbody tr th:only-of-type { vertical-align: middle; }\n",
    ".dataframe tbody tr th {\n",
    "    vertical-align: top;\n",
    "}\n",
    "\n",
    ".dataframe thead th {\n",
    "    text-align: right;\n",
    "}\n",
    "</style>\n",
    "0\tAverage Statistics\tUnnamed: 2\n",
    "0\t2\tJURISDICTION NAME\t10005.800\n",
    "1\t3\tCOUNT PARTICIPANTS\t9.400\n",
    "2\t4\tCOUNT FEMALE\t4.800\n",
    "3\t5\tPERCENT FEMALE\t0.404\n",
    "4\t6\tCOUNT MALE\t4.600\n",
    "# Look at the last five rows\n",
    "df1.tail()\n",
    "<style scoped> .dataframe tbody tr th:only-of-type { vertical-align: middle; }\n",
    ".dataframe tbody tr th {\n",
    "    vertical-align: top;\n",
    "}\n",
    "\n",
    ".dataframe thead th {\n",
    "    text-align: right;\n",
    "}\n",
    "</style>\n",
    "0\tAverage Statistics\tUnnamed: 2\n",
    "40\t42\tCOUNT NRECEIVES PUBLIC ASSISTANCE\t7.100\n",
    "41\t43\tPERCENT NRECEIVES PUBLIC ASSISTANCE\t0.649\n",
    "42\t44\tCOUNT PUBLIC ASSISTANCE UNKNOWN\t0.000\n",
    "43\t45\tPERCENT PUBLIC ASSISTANCE UNKNOWN\t0.000\n",
    "44\t46\tCOUNT PUBLIC ASSISTANCE TOTAL\t9.400\n",
    "# Import the second part of the data\n",
    "df2 = pd.read_csv('Data/Zipcode_Demos.csv', skiprows=47)\n",
    "df2.head()\n",
    "<style scoped> .dataframe tbody tr th:only-of-type { vertical-align: middle; }\n",
    ".dataframe tbody tr th {\n",
    "    vertical-align: top;\n",
    "}\n",
    "\n",
    ".dataframe thead th {\n",
    "    text-align: right;\n",
    "}\n",
    "</style>\n",
    "47\tJURISDICTION NAME\tCOUNT PARTICIPANTS\tCOUNT FEMALE\tPERCENT FEMALE\tCOUNT MALE\tPERCENT MALE\tCOUNT GENDER UNKNOWN\tPERCENT GENDER UNKNOWN\tCOUNT GENDER TOTAL\t...\tCOUNT CITIZEN STATUS TOTAL\tPERCENT CITIZEN STATUS TOTAL\tCOUNT RECEIVES PUBLIC ASSISTANCE\tPERCENT RECEIVES PUBLIC ASSISTANCE\tCOUNT NRECEIVES PUBLIC ASSISTANCE\tPERCENT NRECEIVES PUBLIC ASSISTANCE\tCOUNT PUBLIC ASSISTANCE UNKNOWN\tPERCENT PUBLIC ASSISTANCE UNKNOWN\tCOUNT PUBLIC ASSISTANCE TOTAL\tPERCENT PUBLIC ASSISTANCE TOTAL\n",
    "0\t48\t10001\t44\t22\t0.50\t22\t0.50\t0\t0\t44\t...\t44\t100\t20\t0.45\t24\t0.55\t0\t0\t44\t100\n",
    "1\t49\t10002\t35\t19\t0.54\t16\t0.46\t0\t0\t35\t...\t35\t100\t2\t0.06\t33\t0.94\t0\t0\t35\t100\n",
    "2\t50\t10003\t1\t1\t1.00\t0\t0.00\t0\t0\t1\t...\t1\t100\t0\t0.00\t1\t1.00\t0\t0\t1\t100\n",
    "3\t51\t10004\t0\t0\t0.00\t0\t0.00\t0\t0\t0\t...\t0\t0\t0\t0.00\t0\t0.00\t0\t0\t0\t0\n",
    "4\t52\t10005\t2\t2\t1.00\t0\t0.00\t0\t0\t2\t...\t2\t100\t0\t0.00\t2\t1.00\t0\t0\t2\t100\n",
    "5 rows × 47 columns\n",
    "\n",
    "Level Up (Optional) - Loading Corrupt CSV files\n",
    "Occasionally, you encounter some really ill-formatted data. One example of this can be data that has strings containing commas in a csv file. Under the standard protocol, when this occurs, one is supposed to use quotes to differentiate between the commas denoting fields and the commas within those fields themselves. For example, we could have a table like this:\n",
    "\n",
    "ReviewerID,Rating,N_reviews,Review,VenueID 123456,4,137,This restaurant was pretty good, we had a great time.,98765\n",
    "\n",
    "Which should be saved like this if it were a csv (to avoid confusion with the commas in the Review text): \"ReviewerID\",\"Rating\",\"N_reviews\",\"Review\",\"VenueID\" \"123456\",\"4\",\"137\",\"This restaurant was pretty good, we had a great time.\",\"98765\"\n",
    "\n",
    "Attempt to import the corrupt file, or at least a small preview of it. It is appropriately titled 'Yelp_Reviews_corrupt.csv'. Investigate some of the intricacies of skipping rows to then pass over this error and comment on what you think is going on.\n",
    "\n",
    "# Your code here\n",
    "try:\n",
    "    df = pd.read_csv('Data/Yelp_Reviews_Corrupt.csv')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "Error tokenizing data. C error: Expected 10 fields in line 2331, saw 11\n",
    "# # Iteration 1 \n",
    "for i in range(1500,2000):\n",
    "    try:\n",
    "        df = pd.read_csv('Data/Yelp_Reviews_Corrupt.csv', nrows=i)\n",
    "    except:\n",
    "        print('First failure at: {}'.format(i))\n",
    "        break\n",
    "df1 = pd.read_csv('Data/Yelp_Reviews_Corrupt.csv', nrows=i-1)\n",
    "print(len(df))\n",
    "df1.head()\n",
    "First failure at: 1962\n",
    "1961\n",
    "<style scoped> .dataframe tbody tr th:only-of-type { vertical-align: middle; }\n",
    ".dataframe tbody tr th {\n",
    "    vertical-align: top;\n",
    "}\n",
    "\n",
    ".dataframe thead th {\n",
    "    text-align: right;\n",
    "}\n",
    "</style>\n",
    "Unnamed: 0\tbusiness_id\tcool\tdate\tfunny\treview_id\tstars\ttext\tuseful\tuser_id\n",
    "0\t1\tpomGBqfbxcqPv14c3XH-ZQ\t0\t2012-11-13\t0.0\tdDl8zu1vWPdKGihJrwQbpw\t5.0\tI love this place! My fiance And I go here atl...\t0.0\tmsQe1u7Z_XuqjGoqhB0J5g\n",
    "1\t2\tjtQARsP6P-LbkyjbO1qNGg\t1\t2014-10-23\t1.0\tLZp4UX5zK3e-c5ZGSeo3kA\t1.0\tTerrible. Dry corn bread. Rib tips were all fa...\t3.0\tmsQe1u7Z_XuqjGoqhB0J5g\n",
    "2\t4\tUms3gaP2qM3W1XcA5r6SsQ\t0\t2014-09-05\t0.0\tjsDu6QEJHbwP2Blom1PLCA\t5.0\tDelicious healthy food. The steak is amazing. ...\t0.0\tmsQe1u7Z_XuqjGoqhB0J5g\n",
    "3\t5\tvgfcTvK81oD4r50NMjU2Ag\t0\t2011-02-25\t0.0\tpfavA0hr3nyqO61oupj-lA\t1.0\tThis place sucks. The customer service is horr...\t2.0\tmsQe1u7Z_XuqjGoqhB0J5g\n",
    "4\t10\tyFumR3CWzpfvTH2FCthvVw\t0\t2016-06-15\t0.0\tSTiFMww2z31siPY7BWNC2g\t5.0\tI have been an Emerald Club member for a numbe...\t0.0\tTlvV-xJhmh7LCwJYXkV-cg\n",
    "df1.tail()\n",
    "<style scoped> .dataframe tbody tr th:only-of-type { vertical-align: middle; }\n",
    ".dataframe tbody tr th {\n",
    "    vertical-align: top;\n",
    "}\n",
    "\n",
    ".dataframe thead th {\n",
    "    text-align: right;\n",
    "}\n",
    "</style>\n",
    "Unnamed: 0\tbusiness_id\tcool\tdate\tfunny\treview_id\tstars\ttext\tuseful\tuser_id\n",
    "1956\t4993\tu8C8pRvaHXg3PgDrsUHJHQ\t0\t2016-08-08\t0.0\tgXmHGBSBBz2-uHdvGf4lZQ\t2.0\tjust went to a retirement party upstairs and t...\t1.0\ttFa-r1pxZh04FjxNSEQgcQ\n",
    "1957\t4998\t-9nai28tnoylwViuJVrYEQ\t0\t2015-03-22\t0.0\tu-zqCN_IXfypJIUzIVUuzw\t5.0\tGreat restaurant and great atmosphere.\tNaN\tNaN\n",
    "1958\tI had an awesome great time with friends.\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "1959\tI loved the tapas and the excellent paella.\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "1960\tI can't wait to come back soon.\t0\totDVyX37h61WEbqPLEjCmQ\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "# # Iteration 2 \n",
    "for i in range(0,500):\n",
    "    try:\n",
    "        temp = pd.read_csv('Data/Yelp_Reviews_Corrupt.csv', skiprows=1962, nrows=i, names=df1.columns)\n",
    "    except:\n",
    "        print('First failure at: {}'.format(i))\n",
    "        break\n",
    "df2 = pd.read_csv('Data/Yelp_Reviews_Corrupt.csv', skiprows=1962, nrows=i-1, names=df1.columns)\n",
    "print(len(df2))\n",
    "df2.head()\n",
    "498\n",
    "<style scoped> .dataframe tbody tr th:only-of-type { vertical-align: middle; }\n",
    ".dataframe tbody tr th {\n",
    "    vertical-align: top;\n",
    "}\n",
    "\n",
    ".dataframe thead th {\n",
    "    text-align: right;\n",
    "}\n",
    "</style>\n",
    "Unnamed: 0\tbusiness_id\tcool\tdate\tfunny\treview_id\tstars\ttext\tuseful\tuser_id\n",
    "0\tSTAY AWAY FROM THIS PLACE!!!!!!\t5\tsDofYImMQQmu4Le5G9zmpQ\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "1\t3948\tGAKFx4jFUtTOTpp_jDJnuA\t0\t2017-09-01\t0\tOUZWMw7EgO7D596pUelSlA\t5\tNice relaxing atmosphere. Friendly service and...\t1\t6vJY67yve43Ijvn8RKVUow\n",
    "2\t3949\t0QzCeORfF8EY34UODWRV9A\t0\t2017-09-03\t0\t7lbykaWFD8YBwT0mU1Rexw\t4\tVery pleased with our experience. Great off th...\t0\t6vJY67yve43Ijvn8RKVUow\n",
    "3\t3950\ttlt8zNrZ6_A3DmXiM-cnBA\t0\t2016-06-12\t0\tNd_soHwCYi8adcNIT2w9LQ\t1\tWife went to this location and was horrible. N...\t0\tS0dnPb1OzaqdBSOxyLr7BQ\n",
    "4\t3952\tXD0LjNuPPwJPsTAHecUh7A\t0\t2015-08-23\t0\tFUUTAr5CECrkfRa9Y2-MSg\t1\tNot baby friendly anymore.\tNaN\tNaN\n",
    "temp = pd.read_csv('Data/Yelp_Reviews_Corrupt.csv')\n",
    "print(len(temp))\n",
    "temp.head()\n",
    "---------------------------------------------------------------------------\n",
    "\n",
    "ParserError                               Traceback (most recent call last)\n",
    "\n",
    "<ipython-input-15-d6af0e7ded24> in <module>()\n",
    "      1 # __SOLUTION__\n",
    "----> 2 temp = pd.read_csv('Data/Yelp_Reviews_Corrupt.csv')\n",
    "      3 print(len(temp))\n",
    "      4 temp.head()\n",
    "\n",
    "\n",
    "~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\n",
    "    683         )\n",
    "    684 \n",
    "--> 685         return _read(filepath_or_buffer, kwds)\n",
    "    686 \n",
    "    687     parser_f.__name__ = name\n",
    "\n",
    "\n",
    "~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds)\n",
    "    461 \n",
    "    462     try:\n",
    "--> 463         data = parser.read(nrows)\n",
    "    464     finally:\n",
    "    465         parser.close()\n",
    "\n",
    "\n",
    "~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py in read(self, nrows)\n",
    "   1152     def read(self, nrows=None):\n",
    "   1153         nrows = _validate_integer(\"nrows\", nrows)\n",
    "-> 1154         ret = self._engine.read(nrows)\n",
    "   1155 \n",
    "   1156         # May alter columns / col_dict\n",
    "\n",
    "\n",
    "~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py in read(self, nrows)\n",
    "   2046     def read(self, nrows=None):\n",
    "   2047         try:\n",
    "-> 2048             data = self._reader.read(nrows)\n",
    "   2049         except StopIteration:\n",
    "   2050             if self._first_chunk:\n",
    "\n",
    "\n",
    "pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.read()\n",
    "\n",
    "\n",
    "pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader._read_low_memory()\n",
    "\n",
    "\n",
    "pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader._read_rows()\n",
    "\n",
    "\n",
    "pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader._tokenize_rows()\n",
    "\n",
    "\n",
    "pandas/_libs/parsers.pyx in pandas._libs.parsers.raise_parser_error()\n",
    "\n",
    "\n",
    "ParserError: Error tokenizing data. C error: Expected 10 fields in line 2331, saw 11\n",
    "temp = pd.read_csv('Data/Yelp_Reviews_Corrupt.csv', names=df1.columns, skiprows=1)\n",
    "print(len(temp))\n",
    "temp.head()\n",
    "4651\n",
    "<style scoped> .dataframe tbody tr th:only-of-type { vertical-align: middle; }\n",
    ".dataframe tbody tr th {\n",
    "    vertical-align: top;\n",
    "}\n",
    "\n",
    ".dataframe thead th {\n",
    "    text-align: right;\n",
    "}\n",
    "</style>\n",
    "Unnamed: 0\tbusiness_id\tcool\tdate\tfunny\treview_id\tstars\ttext\tuseful\tuser_id\n",
    "0\t1\tpomGBqfbxcqPv14c3XH-ZQ\t0\t2012-11-13\t0\tdDl8zu1vWPdKGihJrwQbpw\t5\tI love this place! My fiance And I go here atl...\t0\tmsQe1u7Z_XuqjGoqhB0J5g\n",
    "1\t2\tjtQARsP6P-LbkyjbO1qNGg\t1\t2014-10-23\t1\tLZp4UX5zK3e-c5ZGSeo3kA\t1\tTerrible. Dry corn bread. Rib tips were all fa...\t3\tmsQe1u7Z_XuqjGoqhB0J5g\n",
    "2\t4\tUms3gaP2qM3W1XcA5r6SsQ\t0\t2014-09-05\t0\tjsDu6QEJHbwP2Blom1PLCA\t5\tDelicious healthy food. The steak is amazing. ...\t0\tmsQe1u7Z_XuqjGoqhB0J5g\n",
    "3\t5\tvgfcTvK81oD4r50NMjU2Ag\t0\t2011-02-25\t0\tpfavA0hr3nyqO61oupj-lA\t1\tThis place sucks. The customer service is horr...\t2\tmsQe1u7Z_XuqjGoqhB0J5g\n",
    "4\t10\tyFumR3CWzpfvTH2FCthvVw\t0\t2016-06-15\t0\tSTiFMww2z31siPY7BWNC2g\t5\tI have been an Emerald Club member for a numbe...\t0\tTlvV-xJhmh7LCwJYXkV-cg\n",
    "pd.read_csv('Data/Yelp_Reviews_Corrupt.csv', skiprows=len(df1)+len(df2), names=df1.columns)\n",
    "<style scoped> .dataframe tbody tr th:only-of-type { vertical-align: middle; }\n",
    ".dataframe tbody tr th {\n",
    "    vertical-align: top;\n",
    "}\n",
    "\n",
    ".dataframe thead th {\n",
    "    text-align: right;\n",
    "}\n",
    "</style>\n",
    "Unnamed: 0\tbusiness_id\tcool\tdate\tfunny\treview_id\tstars\ttext\tuseful\tuser_id\n",
    "0\tCons:\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "1\t- Dusty! Not sure if it's all of Vegas but I...\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "2\t- Valet parking: kinda inconvenient when you ...\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "3\t- Sofabed is extremely flimsy\tif you have more than 2 people\tinsist on 2 queen beds. the sofa cushions ar...\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "4\tOther points:\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "5\t* Should call ahead of time to make sure your...\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "6\t* Hotel lobby is extremely small!\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "7\t* In-room food service was overpriced (and fo...\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "8\t* Don't go to the 7-11\tit's shady. You can shop at the am/pm or the...\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "9\tOverall\tit was a good experience for the price we pai...\t3\tDZYGeWwBRKHgLUSk12sCvA\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "10\t4058\tWPCgtEG-bJt0cZtnM-x7yw\t0\t2012-02-28\t0\tigf8qa4uqeApRYwnrmcnWg\t5\tLoud\tfun and full of excitement! This high energy\taudience participation show was awesome. Gre...\n",
    "11\t1624\tT5R6aILLDBnHQvfejY7dgA\t1\t2012-07-09\t0\tHjYi1MBvuVf8fVsLeLx1bQ\t5\tI've gone here since I was 8 or 9 years old. N...\tI plan on taking my future children there. I ...\t2\n",
    "12\t4938\t53BSdnhzcCBfBH_6TgX63Q\t0\t2014-08-31\t0\thHVBKv4nacYCphHhHt2KIA\t5\tVery courteous service\tvery delicious cuisine\tand a very phenomenal experience over all. I ...\n",
    "13\t2897\thOB3NHuF-iVFdEkrA-PUlg\t1\t2012-02-17\t0\tacsWXRjRWWKzITQK4KaI4w\t4\tThe Gault is a wonderful destination. My fian...\tbut due to the cozy warmth of our home-away-f...\tNaN\n",
    "14\tThe room was lovely\twe had a loft basic package and really enjoye...\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "15\tWe took part in the spa package offered throug...\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "16\tI emailed the concierge ahead of time to reque...\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "17\tI've given 4 stars instead of 5 for a couple o...\tthe breakfast is not included in the room rat...\tand the gym is adequate but quite small.\t3\tgIWWW6w-6P2j-hTH7nantw\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "18\t2422\tvwQvDIb_F7AqwCPaQhHrwg\t0\t2012-10-29\t8\tiN83pl9IPEo6Ay8g_G5g5g\t1\tIf you are looking for an over-priced dorm lik...\tunderage drinking\tand plenty of side-stepping the puke left in ...\n",
    "19\t1527\t04u-szAykldu-caSDHQaKA\t0\t2012-02-09\t0\tVkEeSwaqHgp6VFeANMoFow\t4\tI have been looking for a good Chinese restrai...\t0\tiNSL4q8MUvZ1ItVGboPpbQ\n",
    "20\t4080\tEDcZRvERC22Cvw1yi4-VKg\t1\t2017-12-05\t0\ttTb_HmUXAj5UwxX6kh8k1w\t2\tIts was ok. I got the sausage meatballs and Ar...\talthough the description said square. Friend ...\tclean\n",
    "21\t4237\tZM-ljL_Y6bR4qEYsGHws5A\t0\t2016-11-05\t0\tbl6WJnhCl0s1Jd4TUcSX_g\t2\tCalled to see if they had availability and a g...\tso come right over. I said I was a 10 minute...\t0\n",
    "22\t4498\tVRTfAP2DjvUYxRY3dw37hA\t0\t2014-01-08\t0\t3CYH_03G3ZtIHoA3gIWpLA\t4\tLuxurious. But of course\tyou'd expect that from the Bellagio.\tNaN\n",
    "23\tChi\tmy pedicurist\twas wonderful. Super sweet and very attentive...\tit was heavenly. When it was all over\tI was kicking myself for not scheduling addit...\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "24\tFour stars instead of five because:\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "25\t- The ladies in reception were a bit rude\tboth over the phone and more so in person.\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "26\t- My pedicure only lasted two days! A pedicure...\teven 3 weeks without a single chip.\t7\tETmpBain2s02PqHGwSr7hQ\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "27\t2716\t4VHp2gei1bpY68ZzEZE9Bg\t2\t2013-05-22\t9\tm0eO3358SKkzY7isf6kEpg\t1\tNot impressed at all. I paid 200 to go up and ...\twhich was good. Unfortunately\tthe other 1/2 was spent bad mouthing the comp...\n",
    "28\t3426\t8g3u6g7J93nIOF8owARxew\t0\t2014-08-13\t0\tJVwa-qaFERoa2dg0poiBcg\t3\tIf you're looking for a shawarma in this area\tthis is your only option. Better tasting opti...\tNaN\n",
    "29\tHad the traditional chicken shawarma. one of m...\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\n",
    "2552\tStarting off with drinks\tBamburger serves beer\twine\tand old-fashioned Stewart's soda\tbut what is a burger joint without milkshakes...\tincluding the usual vanilla and chocolate sus...\tas well a special flavour each day for the ni...\tNaN\tNaN\tNaN\n",
    "2553\tThe menu is varied\toffering up soup\tsalads\tsandwiches and desserts\tas well as a kid's menu\tbut almost everyone here is in it for the bur...\tfrom the type of bun\tburger patty (beef\tchicken\tturkey\n",
    "2554\tWe went with the Bambamburger ($11.50)\twhich is 2/3 of a pound of prime ground chuck\tand the chicken burger ($9.95) for myself\ton whole-wheat buns. Both of us outfitted our...\tincluding mushrooms\tonions\tgarlic mayo and cheese sauce for the Bambambu...\tand avocado\tchipotle mayo\tdill pickles and jalapeno peppers for the chi...\n",
    "2555\tBamburger serves up great burgers\tfries and shakes at a fairly good price\talthough if you go a little overboard with th...\tyou might quickly end up with a $20 burger\twithout realizing it. The service was friendl...\tand the atmosphere was very comfortable and n...\tNaN\tNaN\tNaN\tNaN\n",
    "2556\tIf you are hunting for a real deal on a burger\tBamburger might not be what you are looking for\tbut if you are more on the adventurous side\tand want to have fun creating your own burger...\tBamburger will deliver.\t8\tYHWsLBS8jzZiPjKHMFOaAA\tNaN\tNaN\tNaN\n",
    "2557\t3225\tiyyWYpWm8X-6i7kBR3JHuw\t0\t2014-01-27\t0\tyLKMQNn8VE3CEDX-TF5CfA\t1\tTried to attend a recent basketball game. Purc...\ttwo parents and two kids. Arrived 45 min earl...\t1\n",
    "2558\t4674\t4KfDcE9iU2isFpoaKeDpgw\t0\t2012-06-14\t0\trFH9iSvRmdm5LtDdsYwwpA\t5\tGreat place to take your kids and interesting ...\t0\t9gYbRvijurhrnC6yPRlaUw\n",
    "2559\t4719\tP4Plzlfm4uJjNmH3wY4W1Q\t0\t2014-01-14\t0\tIobIRp1mGJiLg4B6wHAvqQ\t2\tWhile I will continue to eat at this establish...\tI will only do so because it is the closet Ch...\totherwise I would completely avoid this place...\n",
    "2560\t3440\tnW45ez1L6U4PsYhV1BTrGQ\t0\t2012-05-19\t0\tabStF7f3_IyfZmOP82baZQ\t5\tH&F Jewellery is amazing!\tNaN\tNaN\n",
    "2561\tI had given my husband some ideas of engagemen...\ta friend of ours recommended H&F.\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "2562\tThe customer service here is great - they are ...\tattentive\thonest and the prices are very reasonable! My...\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "2563\tWe went back for our wedding bands and I worke...\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "2564\tWe've recommended 5 other friends to H&F - the...\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "2565\tGo to H&F for all your jewellery needs - you w...\t1\tPkRFSQgSfca9Tamq7b2LdQ\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "2566\t4812\te13SEvJud_vgeDR_doL4sQ\t0\t2013-03-01\t0\tlUV1KEm4cl4mluSEFharFQ\t4\tWe loved the food. The service was good and we...\t0\tNW6gHZ8PlYl1STK1A1Ixeg\n",
    "2567\t1970\t9NBkIExYYz3w9O5JdzDOMA\t0\t2013-11-03\t0\tRT20_fUNJJNqMNvvjviNbQ\t1\tHit or miss. I used these guys twice in a day ...\tno call nothing. So I call the dispatch to le...\t$20 to go 4 miles? I'll be renting a car tomo...\n",
    "2568\t689\tBTcY04QFiS1uh-RpkR7rAg\t1\t2013-06-02\t0\t6_A58CCY8SHB7r-Wu7-A5g\t5\tCame here with my 2 year old daughter for our ...\tshe asked if everything was ok. We old her w...\tgreat seating!\n",
    "2569\t4874\tt0T_4MM4EUHbCzBTF11FHA\t0\t2016-08-14\t0\tKqQwNyfoFiJOw911mrULIg\t5\tGreat little restaurant. Not to many tables an...\twhich is awesome. We had the Pad Thai and the...\t0\n",
    "2570\t564\t5XYR6doRa5Nj1JMfSDei6A\t1\t2016-06-14\t0\txlGJkxoIBl8XH8wVsPZpnw\t5\tAlways great friendly service and fresh baked ...\tNaN\tNaN\n",
    "2571\tHighly recommend the custard cakes they are th...\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "2572\tThe rice flour cake is also really good and a ...\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "2573\tThe bean cakes are great here too! orange\talmond\tand a few others I have tried are all good.\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "2574\tCan't go wrong with Nova Era\t0\tkBNFdviedCPFWyR-wVaAzw\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "2575\t3458\taLcFhMe6DDJ430zelCpd2A\t0\t2013-10-02\t0\tkwiEG_KCpDB6aK5fTSM7iw\t2\tWe were expecting amazing Thai food after all ...\tNaN\tNaN\n",
    "2576\tThis was disappointing.\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "2577\tFirst off\tit was really awkward sitting on the benches ...\tas people walked past us while to wait for ou...\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "2578\tSecond\twhen we were seated\tit was so loud. It felt like we were in a hig...\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "2579\tFinally - Food was mediocre. I was extremely d...\tbut it wasn't flavourful.\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "2580\tWasn't worth the hype\tunfortunately.\t1\tPkRFSQgSfca9Tamq7b2LdQ\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\n",
    "2581\t4206\tWdBWhGe4Siqg3IYTc4_K4A\t0\t2016-08-15\t0\tO0ttxNGxHKtD8Cnnwc_j1g\t1\tSunday at 8p. Not many people here at all. We ...\tand no one came to take our drink order. We w...\t0\n",
    "2582 rows × 10 columns\n",
    "\n",
    "Summary\n",
    "Congratulations, you now practiced your Pandas-importing skills!\n",
    "\n",
    "© 2020 GitHub, Inc.\n",
    "Terms\n",
    "Privacy\n",
    "Security\n",
    "Status\n",
    "Help\n",
    "Contact GitHub\n",
    "Pricing\n",
    "API\n",
    "Training\n",
    "Blog\n",
    "About\n"
   ]
  }
 ]
}